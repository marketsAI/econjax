{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VFI brute force Benchmarking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOkV30C/TyxQAUQAxgweDHT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasCovarrubias/econjax/blob/main/VFI_brute_force_Benchmarking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr2uTeyMc-w_",
        "outputId": "df9971ca-ee92-4f91-81e4-9937a4af5e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: quantecon in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from quantecon) (2.23.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.7.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from quantecon) (0.51.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->quantecon) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->quantecon) (0.34.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (1.24.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->quantecon) (1.2.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.008155345916748047 sec\n",
            "WARNING:absl:Compiling T_manualvec (140437705599024 for args (ShapedArray(float32[1024,128]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.6419801712036133 sec\n",
            "WARNING:absl:Finished tracing + transforming T_autovec for jit in 0.03256821632385254 sec\n",
            "WARNING:absl:Compiling T_autovec (140434552330288 for args (ShapedArray(float32[1024,128]),).\n",
            "WARNING:absl:Finished XLA compilation of T_autovec in 0.7977650165557861 sec\n",
            "WARNING:absl:Finished tracing + transforming T_tpu for pmap in 0.045196533203125 sec\n",
            "WARNING:absl:Compiling T_tpu (140434551730448) for 8 devices with args [ShapedArray(int32[8,128]), ShapedArray(float32[1024,128])]. (num_replicas=8 num_partitions=1)\n",
            "WARNING:absl:Finished XLA compilation of T_tpu in 0.4486241340637207 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.002180337905883789 sec\n",
            "WARNING:absl:Compiling _multi_slice (140434536639472 for args (ShapedArray(int32[8,128]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.029089689254760742 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.01114964485168457 sec\n",
            "WARNING:absl:Compiling T_manualvec (140434552815248 for args (ShapedArray(float32[2048,256]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.6825742721557617 sec\n",
            "WARNING:absl:Finished tracing + transforming T_autovec for jit in 0.029296159744262695 sec\n",
            "WARNING:absl:Compiling T_autovec (140437705600464 for args (ShapedArray(float32[2048,256]),).\n",
            "WARNING:absl:Finished XLA compilation of T_autovec in 0.8129279613494873 sec\n",
            "WARNING:absl:Finished tracing + transforming T_tpu for pmap in 0.02789616584777832 sec\n",
            "WARNING:absl:Compiling T_tpu (140434552074064) for 8 devices with args [ShapedArray(int32[8,256]), ShapedArray(float32[2048,256])]. (num_replicas=8 num_partitions=1)\n",
            "WARNING:absl:Finished XLA compilation of T_tpu in 0.7329978942871094 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.003522634506225586 sec\n",
            "WARNING:absl:Compiling _multi_slice (140434552072624 for args (ShapedArray(int32[8,256]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.0417780876159668 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Size of grid': [131072, 524288], 'Manual Vectorization': [0.0030656488999966313, 0.014602750199992442], 'Automatic Vectorization': [0.0031134629999996833, 0.015370856399999865], 'TPU Parallelization': [0.015546994400006042, 0.02057383479998407]}\n"
          ]
        }
      ],
      "source": [
        "# installs\n",
        "!pip install -U quantecon # Install quantecon in case it's missing\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from numba import njit\n",
        "import quantecon as qe \n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "import json\n",
        "import argparse\n",
        "from jax.config import config\n",
        "config.update(\"jax_log_compiles\", 1)\n",
        "# to suppress watnings uncomment next two lines\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "# MANUAL VECTORIZATION\n",
        "\n",
        "def get_T_manualvec(model: dict):\n",
        "  params = model[\"params\"]\n",
        "  a = model[\"batched_grids\"][\"a\"]\n",
        "  y = model[\"batched_grids\"][\"y\"]\n",
        "  ap = model[\"batched_grids\"][\"ap\"]\n",
        "  P = model[\"batched_grids\"][\"P\"]\n",
        "  \n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_manualvec(v):\n",
        "      vp = jnp.dot(v, P) # vp has shape (a_size, y_size, 1)\n",
        "      c = params[\"R\"] * a + y - ap # c has shape (a_size, y_size, ap_size)\n",
        "      # m = jnp.where(c > 0, u(c) + β * vp, -np.inf) # m has shape (a_size, y_size, ap_size) \n",
        "      m = u(c) + params[\"beta\"] * vp # m has shape (a_size, y_size, ap_size) \n",
        "      return jnp.max(m, axis=2) # we to average over the last axis, that is, for each a and y, get the the max over ap.\n",
        "    \n",
        "  return T_manualvec\n",
        "\n",
        "\n",
        "# AUTOMATIMC VECTORIZATION\n",
        "\n",
        "\n",
        "def get_T_autovec(model:dict):\n",
        "  params = model[\"params\"]\n",
        "  grids = model[\"grids\"]\n",
        "  P = model[\"Trans_matrix\"]\n",
        "  \n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_autovec(v: jnp.ndarray):\n",
        "\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      # \n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:],P[y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = model[\"indices\"][\"a\"]\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_state_value = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    return new_state_value\n",
        "\n",
        "  return T_autovec\n",
        "\n",
        "# TPU PARALLELIZATION\n",
        "def get_T_tpu(model:dict):\n",
        "\n",
        "  params = model[\"params\"]\n",
        "  grids = model[\"grids\"]\n",
        "  P = model[\"Trans_matrix\"]\n",
        "  \n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_tpu(a_partition:jnp.array, v: jnp.ndarray):\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      # \n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:], model[\"Trans_matrix\"][y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = a_partition\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_state_value = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    return new_state_value\n",
        "\n",
        "  return T_tpu\n",
        "\n",
        "# BENCHMARKING\n",
        "\n",
        "\n",
        "def main(use_TPU=True):\n",
        "\n",
        "  if use_TPU:\n",
        "      import jax.tools.colab_tpu\n",
        "      jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "  # Global Parameteres\n",
        "  params = {\n",
        "      \"R\": 1.1,\n",
        "      \"beta\": 0.99,\n",
        "      \"gamma\": 2.5\n",
        "  }\n",
        "  a_min, a_max = 0.01, 2\n",
        "  ρ = 0.9\n",
        "  σ = 0.1\n",
        "  n_devices = jax.local_device_count()\n",
        "\n",
        "  # Loop over scale levels\n",
        "  results_dict = {\"Size of grid\": [], \"Manual Vectorization\": [], \"Automatic Vectorization\": [], \"TPU Parallelization\": []}\n",
        "  for scale in [1, 4, 8, 16, 32]:\n",
        "    # grid for assets\n",
        "    a_size = ap_size = 1024*scale\n",
        "    a_grid = jnp.linspace(a_min, a_max, a_size)  # grid for a\n",
        "    ap_grid = jnp.linspace(a_min, a_max, a_size)                 # grid for a'\n",
        "    #grid for y (use QuantEcon's tauchen() function to create Markov Chains out of AR(1))\n",
        "\n",
        "    y_size = 128*scale\n",
        "    mc = qe.tauchen(ρ, σ, n=y_size)\n",
        "    y_grid = jnp.exp(mc.state_values)\n",
        "    P = jnp.array(mc.P)\n",
        "    results_dict[\"Size of grid\"].append(a_size*y_size)\n",
        "\n",
        "    grids = {\n",
        "      \"a\": a_grid,\n",
        "      \"y\": y_grid,\n",
        "      \"ap\": ap_grid,}\n",
        "\n",
        "    batched_grids = {\n",
        "      \"P\": jnp.reshape(P, (y_size, y_size, 1)),\n",
        "      \"a\": jnp.reshape(a_grid, (a_size, 1, 1)),\n",
        "      \"y\": jnp.reshape(y_grid, (1, y_size, 1)),\n",
        "      \"ap\": jnp.reshape(ap_grid, (1, 1, ap_size)),\n",
        "      } \n",
        "\n",
        "    model = {\"params\": params, \n",
        "            \"grids\": grids,\n",
        "            \"batched_grids\": batched_grids,\n",
        "            \"Trans_matrix\": P, \n",
        "            \"indices\": {\"a\": jnp.array(range(a_size)), \"y\": jnp.array(range(y_size)), \"ap\": jnp.array(range(ap_size))}\n",
        "            }\n",
        "\n",
        "    # initial value\n",
        "    global v_init\n",
        "    v_init = jnp.zeros((a_size, y_size))\n",
        "\n",
        "    # Get and compile T functions\n",
        "    T_manualvec = get_T_manualvec(model)\n",
        "    global T_manualvec_jit\n",
        "    T_manualvec_jit = jax.jit(T_manualvec).lower(v_init).compile()\n",
        "    T_autovec = get_T_autovec(model)\n",
        "    global T_autovec_jit\n",
        "    T_autovec_jit = jax.jit(T_autovec).lower(v_init).compile()\n",
        "\n",
        "    # run for 10 times and time it using timeit\n",
        "    global a_partitions\n",
        "    devices = jax.local_devices()\n",
        "    a_partitions = jnp.reshape(model[\"indices\"][\"a\"], (n_devices, a_size//n_devices))\n",
        "    #a_partitions = jax.device_put_sharded(a_partitions, devices)\n",
        "    T_tpu = get_T_tpu(model)\n",
        "    global T_tpu_jit\n",
        "    T_tpu_jit = jax.pmap(T_tpu, in_axes = (0,None)).lower(a_partitions, v_init).compile()\n",
        "    results_dict[\"Manual Vectorization\"].append(timeit.timeit('T_manualvec_jit(v_init).block_until_ready()', globals=globals(), number=10)/10)\n",
        "    results_dict[\"Automatic Vectorization\"].append(timeit.timeit('T_autovec_jit(v_init).block_until_ready()', globals=globals(), number=10)/10)\n",
        "    results_dict[\"TPU Parallelization\"].append(timeit.timeit('T_tpu_jit(a_partitions, v_init).block_until_ready()', globals=globals(), number=10)/10)\n",
        "\n",
        "  print(results_dict)\n",
        "\n",
        "main()\n"
      ]
    }
  ]
}