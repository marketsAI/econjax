{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VFI using JAX.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Utsxm-o9SUGI"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyME2iXgvJZDRMsKaynezlZg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasCovarrubias/econjax/blob/main/VFI_using_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parallelized Value Function Iteration Using JAX"
      ],
      "metadata": {
        "id": "9hDVZWighqhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab notebook explores ideas on how to optimally use JAX to perdorm value funciton iteration. The objective is to improve on https://notes.quantecon.org/submission/622ed4daf57192000f918c61 . Hypothetically, we can get better performance by using vmap, making all the fuctions pure, using lax.scan for iterations and using the most optimal jax functions."
      ],
      "metadata": {
        "id": "CRO5lTwrhyCp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuQOCHhqiXl5"
      },
      "source": [
        "The problem is to maximize the expected discounted sum\n",
        "\n",
        "$$ ùîº\\sum_{t‚â•0} Œ≤^t u(c_t)$$\n",
        "\n",
        "subject to\n",
        "\n",
        "$$c_t+a_t+1‚â§Ra_t+yt, \\quad c_t‚â•0,\\quad a_t‚â•0$$\n",
        "\n",
        "for all $t‚â•0$, with $a_0$ and $y_0$ given. Here $c_t$ is consumption, $a_t$ is assets, $R$ is the gross risk-free rate of return, and $y_t$ is income. The income process follows a Markov chain with transition matrix $P$.\n",
        "\n",
        "The Bellman equation is\n",
        "\n",
        "$$v(a,y)= \\underset{0‚â§a‚Ä≤‚â§Ra+y}{\\max} \\{ u(Ra+y‚àía‚Ä≤)+Œ≤\\sum_{y‚Ä≤}v(a‚Ä≤,y‚Ä≤)P(y,y‚Ä≤) \\}$$\n",
        "\n",
        "where $v$ is the value function. The corresponding Bellman operator is\n",
        "\n",
        "$$Tv(a,y)= \\underset{0‚â§a‚Ä≤‚â§Ra+y}{\\max} \\{ u(Ra+y‚àía‚Ä≤)+Œ≤\\sum_{y‚Ä≤}v(a‚Ä≤,y‚Ä≤)P(y,y‚Ä≤) \\}$$\n",
        "\n",
        "We solve the dynamic program by value function iteration --- that is, by iterating with T.\n"
      ],
      "metadata": {
        "id": "DuQOCHhqiXl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by installing quantecon and importing the libraries we will use"
      ],
      "metadata": {
        "id": "0VGwrYCDnece"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installs\n",
        "!pip install -U quantecon # Install quantecon in case it's missing\n",
        "\n",
        "#imports\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from numba import njit\n",
        "import quantecon as qe \n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "from jax.config import config\n",
        "config.update(\"jax_log_compiles\", 1)\n",
        "# to suppress watnings uncomment next two lines\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "#chage to False if using CPU or GPU runtime.\n",
        "use_TPU = False\n",
        "if use_TPU:\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "  !nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83a744e-9512-4e11-bd4c-faead3b3b18c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting quantecon\n",
            "  Downloading quantecon-0.5.3-py3-none-any.whl (179 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñâ                              | 10 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñã                            | 20 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                          | 30 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 40 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                      | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 81 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç               | 92 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà            | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå      | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 179 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.7.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from quantecon) (0.51.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from quantecon) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->quantecon) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->quantecon) (0.34.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (1.24.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->quantecon) (1.2.1)\n",
            "Installing collected packages: quantecon\n",
            "Successfully installed quantecon-0.5.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 13 16:45:11 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# installs\n",
        "!pip install -U quantecon # Install quantecon in case it's missing\n",
        "\n",
        "#imports\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from numba import njit\n",
        "import quantecon as qe \n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "from jax.config import config\n",
        "config.update(\"jax_log_compiles\", 1)\n",
        "# to suppress watnings uncomment next two lines\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting Point: Naive implementation on a small grid"
      ],
      "metadata": {
        "id": "ijfXQhUfIyqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we specify global parameters (we will then move it to local), the grids for the state and actions, and the inital value for the value function. "
      ],
      "metadata": {
        "id": "u2Joo0ilkTRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R = 1.1\n",
        "Œ≤ = 0.99\n",
        "Œ≥ = 2.5\n",
        "\n",
        "# grid for assets\n",
        "a_min, a_max = 0.01, 2\n",
        "a_size = ap_size = 128\n",
        "a_grid = np.linspace(a_min, a_max, a_size)  # grid for a\n",
        "ap_grid = np.linspace(a_min, a_max, a_size)                 # grid for a'\n",
        "\n",
        "#grid for y (use QuantEcon's tauchen() function to create Markov Chains out of AR(1))\n",
        "œÅ = 0.9\n",
        "œÉ = 0.1\n",
        "y_size = 8\n",
        "mc = qe.tauchen(œÅ, œÉ, n=y_size)\n",
        "y_grid = np.exp(mc.state_values)\n",
        "P = np.array(mc.P)\n",
        "print(\"state size:\", a_size*y_size)\n",
        "\n",
        "# initial value\n",
        "v_init = np.zeros((a_size, y_size))"
      ],
      "metadata": {
        "id": "Fz8O9k67kZxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089ceace-4677-4796-ce00-b96959cd3484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state size: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to start by using a nainve approach that uses for loops extensively. This structure is going t o be the base of our \"authomatically vectorized\" version."
      ],
      "metadata": {
        "id": "2sEZEyoRGinJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyjlPg9DGo7_",
        "outputId": "63007fc9-699c-4fb6-eb29-6cb0c7b11203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 457 ms, sys: 11.9 ms, total: 469 ms\n",
            "Wall time: 458 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second pass: vectorized solution on large grid using JAX and accelerators\n",
        "\n",
        " First, let's check that Google Colab has assigned us a nice GPU."
      ],
      "metadata": {
        "id": "SgoIy7ARltZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to use TPUs, set use_TPU = True in the next cel. If not, the code !nvidia-smi will tell us what GPU has been asigned to us."
      ],
      "metadata": {
        "id": "-q4rf7AnoPr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_TPU = False\n",
        "if use_TPU:\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "  !nvidia-smi"
      ],
      "metadata": {
        "id": "1O8trXkroSxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4956c4-649f-4c77-a125-a80d8786e230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May 12 02:38:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case you chose a GPU runtime, if you don't use Colab Pro, you should get something lika a Tesla K80. Since I am using Colab Pro, I got a Tesla P100."
      ],
      "metadata": {
        "id": "VzQ8Vxo3mBbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to redefine de grids using jax.numpy arrays and we are going to increase the size of the grid."
      ],
      "metadata": {
        "id": "Vm50me2WKYpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grid for assets\n",
        "scale=4\n",
        "a_size = ap_size = 1028*scale\n",
        "a_grid = jnp.linspace(a_min, a_max, a_size)  # grid for a\n",
        "ap_grid = jnp.linspace(a_min, a_max, a_size)                 # grid for a'\n",
        "\n",
        "#grid for y (use QuantEcon's tauchen() function to create Markov Chains out of AR(1))\n",
        "œÅ = 0.9\n",
        "œÉ = 0.1\n",
        "y_size = 128*scale\n",
        "mc = qe.tauchen(œÅ, œÉ, n=y_size)\n",
        "y_grid = jnp.exp(mc.state_values)\n",
        "P = jnp.array(mc.P)\n",
        "print(\"state size:\", a_size*y_size)\n",
        "\n",
        "# initial value\n",
        "v_init = jnp.zeros((a_size, y_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSn2pgvBKN-S",
        "outputId": "92acbf26-37e4-4de6-b5bc-021a60b1a498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.009136199951171875 sec\n",
            "WARNING:absl:Compiling _linspace (140445892217168 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.4287600517272949 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.00047588348388671875 sec\n",
            "WARNING:absl:Compiling <lambda> (140445892217456 for args (ShapedArray(float32[512]),).\n",
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.10872650146484375 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003161430358886719 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00029659271240234375 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00038361549377441406 sec\n",
            "WARNING:absl:Compiling prim_fun (140445904128816 for args (ShapedArray(float32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.0665590763092041 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state size: 2105344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual vectorization: Reshape grids and transition matrix\n",
        "\n",
        "\n",
        "We add dimensions to arrays so that they will be stretched along the new dimensions when placed in arithmetic operations with other arrays that have more elements along those dimensions. This stretching is done by repeating values, which is what we use to replace loops.\n",
        "\n",
        "The next code cell reshapes all arrays to be three-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model[\"batched_grids\"] = {\n",
        "    \"P\": jnp.reshape(P, (y_size, y_size, 1)),\n",
        "    \"a\": jnp.reshape(a_grid, (a_size, 1, 1)),\n",
        "    \"y\": jnp.reshape(y_grid, (1, y_size, 1)),\n",
        "    \"ap\": jnp.reshape(ap_grid, (1, 1, ap_size)),\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee702d0-9253-47a4-b4a4-74c498b06b42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005838871002197266 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140389008 for args (ShapedArray(float32[2,2]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0044863224029541016 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005173683166503906 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140387088 for args (ShapedArray(float32[16]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003246307373046875 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003857612609863281 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140375568 for args (ShapedArray(float32[2]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003700733184814453 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004076957702636719 sec\n",
            "WARNING:absl:Compiling prim_fun (139675140357296 for args (ShapedArray(float32[16]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0031156539916992188 sec\n"
          ]
        }
      ],
      "source": [
        "P = jnp.reshape(P, (y_size, y_size, 1))\n",
        "a = jnp.reshape(a_grid, (a_size, 1, 1))\n",
        "y = jnp.reshape(y_grid, (1, y_size, 1))\n",
        "ap = jnp.reshape(ap_grid, (1, 1, ap_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJBjSuCQm4MY"
      },
      "source": [
        "Now we can implement a vectorized version of the Bellman operator, which calculates the same values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMVrgeNim435"
      },
      "outputs": [],
      "source": [
        "def get_T_manualvec(model: dict):\n",
        "  params = model[\"params\"]\n",
        "  a = model[\"batched_grids\"][\"a\"]\n",
        "  y = model[\"batched_grids\"][\"y\"]\n",
        "  ap = model[\"batched_grids\"][\"ap\"]\n",
        "  P = model[\"batched_grids\"][\"P\"]\n",
        "  \n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_manualvec(v):\n",
        "      vp = jnp.dot(v, P) # vp has shape (a_size, y_size, 1)\n",
        "      c = params[\"R\"] * a + y - ap # c has shape (a_size, y_size, ap_size)\n",
        "      # m = jnp.where(c > 0, u(c) + Œ≤ * vp, -np.inf) # m has shape (a_size, y_size, ap_size) \n",
        "      m = u(c) + params[\"beta\"] * vp # m has shape (a_size, y_size, ap_size) \n",
        "      return jnp.max(m, axis=2) # we to average over the last axis, that is, for each a and y, get the the max over ap.\n",
        "    \n",
        "  return T_manualvec\n"
      ],
      "metadata": {
        "id": "gMVrgeNim435"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4spbHVTm___"
      },
      "source": [
        "Now wer are gonig to precompile the T operation so JAX optimize it use of the hardware."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T_manualvec = get_T_manualvec(model)\n",
        "T_manualvec_jit = jax.jit(T_manualvec).lower(v_init).compile()\n",
        "%time T_manualvec_jit(v_init).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeDrpKwbtsgQ",
        "outputId": "d48147f9-643f-49af-dcf6-f71c98a9de67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.008809804916381836 sec\n",
            "WARNING:absl:Compiling T_manualvec (139825198784240 for args (ShapedArray(float32[16,2]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.05184221267700195 sec\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 451 ¬µs, sys: 3.86 ms, total: 4.31 ms\n",
            "Wall time: 2.62 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeviceArray([[        nan, -0.23726259],\n",
              "             [        nan, -0.21338044],\n",
              "             [        nan, -0.1932523 ],\n",
              "             [        nan, -0.17610094],\n",
              "             [        nan, -0.16134453],\n",
              "             [        nan, -0.14853972],\n",
              "             [        nan, -0.13734336],\n",
              "             [        nan, -0.12748614],\n",
              "             [        nan, -0.11875407],\n",
              "             [        nan, -0.11097524],\n",
              "             [        nan, -0.10401004],\n",
              "             [-0.21771078, -0.09774413],\n",
              "             [-0.19691947, -0.09208304],\n",
              "             [-0.17923889, -0.08694804],\n",
              "             [-0.1640543 , -0.08227319],\n",
              "             [-0.1508989 , -0.07800273]], dtype=float32)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "T_manualvec_jit = jax.jit(T_manualvec).lower(v_init).compile()\n",
        "%time T_manualvec_jit(v_init).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iNU6avAozFx"
      },
      "source": [
        "With a Tesla P100 GPU, we get 30ms. With TPU, we get 16.9"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authomatic vecotrization using vmap\n",
        "\n",
        "To start, we are going to save all the parts we need in a model dictionary to have pure functions"
      ],
      "metadata": {
        "id": "KTHLO-DHMv1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We are going to store the structure of the model in a dictionary\n",
        "grids = {\n",
        "    \"a\": a_grid,\n",
        "    \"y\": y_grid,\n",
        "    \"ap\": ap_grid,}\n",
        "\n",
        "params = {\n",
        "    \"R\": 1.1,\n",
        "    \"beta\": 0.99,\n",
        "    \"gamma\": 2.5\n",
        "}\n",
        "\n",
        "model = {\"params\": params, \n",
        "         \"grids\": grids, \n",
        "         \"Trans_matrix\": P, \n",
        "         \"indices\": {\"a\": jnp.array(range(a_size)), \"y\": jnp.array(range(y_size)), \"ap\": jnp.array(range(ap_size))}}\n",
        "\n"
      ],
      "metadata": {
        "id": "5M2L0rxf6shO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762e69fc-714c-431a-a909-0e3480f73048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00030350685119628906 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00042891502380371094 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn5sngld7KX9"
      },
      "source": [
        "When we calculate the value function at each point of the grid, we want to pass all the possible actions as a vector, and then calculate the maximum. In order to get the actio_value for all the possible actions, we are going to use vmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3zJaJk89npP"
      },
      "outputs": [],
      "source": [
        "def get_T_autovec(model:dict):\n",
        "  params = model[\"params\"]\n",
        "  grids = model[\"grids\"]\n",
        "  P = model[\"Trans_matrix\"]\n",
        "  \n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_autovec(v: jnp.ndarray):\n",
        "\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      # \n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:],P[y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = model[\"indices\"][\"a\"]\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_state_value = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    return new_state_value\n",
        "\n",
        "  return T_autovec"
      ],
      "metadata": {
        "id": "v3zJaJk89npP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T_autovec = get_T_autovec(model)\n",
        "T_autovec_jit = jax.jit(T_autovec).lower(v_init).compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3907a77e-7a1e-431e-e9aa-c486a7d1ef16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming T_autovec for jit in 0.029858827590942383 sec\n",
            "WARNING:absl:Compiling T_autovec (139825186948592 for args (ShapedArray(float32[16,2]),).\n",
            "WARNING:absl:Finished XLA compilation of T_autovec in 0.05750226974487305 sec\n"
          ]
        }
      ],
      "source": [
        "T_autovec = get_update_fn_autovec(model)\n",
        "T_autovec_jit = jax.jit(T_autovec).lower(v_init).compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks6iS0OVONFR",
        "outputId": "c8f0421c-0d58-4672-90f4-bf0a7d06b611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.41 ms, sys: 3.37 ms, total: 4.78 ms\n",
            "Wall time: 2.77 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeviceArray([[-1.8662423 , -0.23726259],\n",
              "             [-1.2739494 , -0.21338044],\n",
              "             [-0.9399282 , -0.1932523 ],\n",
              "             [-0.7300421 , -0.17610094],\n",
              "             [-0.5881006 , -0.16134453],\n",
              "             [-0.48685533, -0.14853972],\n",
              "             [-0.41165787, -0.13734336],\n",
              "             [-0.3540047 , -0.12748614],\n",
              "             [-0.30865595, -0.11875407],\n",
              "             [-0.2722252 , -0.11097524],\n",
              "             [-0.24243681, -0.10401004],\n",
              "             [-0.21771078, -0.09774413],\n",
              "             [-0.19691947, -0.09208304],\n",
              "             [-0.17923889, -0.08694804],\n",
              "             [-0.1640543 , -0.08227319],\n",
              "             [-0.1508989 , -0.07800273]], dtype=float32)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "%time T_autovec_jit(v_init).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyMHJQ4OpAfB"
      },
      "source": [
        "To finish the exercise off, let's iterate until convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqgYaYNFpDNH",
        "outputId": "688b9242-69b4-4a8a-9276-c6ec942e752e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0\n",
            "Iteration 100\n",
            "Iteration 200\n",
            "Iteration 300\n",
            "Iteration 400\n",
            "Iteration 500\n",
            "Iteration 600\n",
            "Iteration 700\n",
            "Iteration 800\n",
            "Iteration 900\n",
            "Iteration 1000\n",
            "Iteration 1100\n",
            "Iteration 1200\n",
            "Iteration 1300\n",
            "Iteration 1400\n",
            "Warning: iteration hit upper bound 1435.\n",
            "CPU times: user 17.3 s, sys: 1min 18s, total: 1min 35s\n",
            "Wall time: 1min 33s\n"
          ]
        }
      ],
      "source": [
        "def vfi_iterator(v_init=v_init, tol=1e-6, max_iter=1435):\n",
        "    error = tol + 1\n",
        "    i = 0\n",
        "    v = v_init\n",
        "    # while error > tol and i < max_iter:\n",
        "    while i < max_iter:\n",
        "        new_v = T_autovec_jit(v)\n",
        "        error = jnp.max(jnp.abs(new_v - v))\n",
        "        v = new_v\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "        i += 1\n",
        "\n",
        "    if i == max_iter:\n",
        "        print(f\"Warning: iteration hit upper bound {max_iter}.\")\n",
        "    else:\n",
        "        print(f\"\\nConverged at iteration {i}.\")\n",
        "    return v\n",
        "\n",
        "%time v = vfi_iterator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2_IS1SzOinP"
      },
      "source": [
        "## Parallelizing across TPU cores using pmaps\n",
        "\n",
        "We are going to make a very small change to our update funciton so we can give a partition of the grid of a as an input and we get can update the value function only on those states. Then, with pmap, we will vectorize that function so we give a n array with n_device partitions of a and it will update those partitions of the state in parallel across TPU_cores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px-H46ldOnOQ"
      },
      "outputs": [],
      "source": [
        "def get_T_tpu(model:dict):\n",
        "\n",
        "  params = model[\"params\"]\n",
        "  grids = model[\"grids\"]\n",
        "  P = model[\"Trans_matrix\"]\n",
        "  \n",
        "  def u(c):\n",
        "      return c**(1-params[\"gamma\"]) / (1-params[\"gamma\"])\n",
        "\n",
        "  def T_tpu(a_partition:jnp.array, v: jnp.ndarray):\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      # \n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:], model[\"Trans_matrix\"][y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = a_partition\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_state_value = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    return new_state_value\n",
        "\n",
        "  return T_tpu"
      ],
      "metadata": {
        "id": "px-H46ldOnOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVlCCZV0PaYT"
      },
      "source": [
        "No, we create an array with the partitions. The dimension of that array should be (n_cores,a_size/n_cores), so the leading axis organize the different partitions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = jax.local_device_count()\n",
        "a_partitions = jnp.reshape(model[\"indices\"][\"a\"], (n_devices, a_size//n_devices))"
      ],
      "metadata": {
        "id": "rky2k2rIPZh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcecc5d2-c85b-4b8e-9758-2a6b73ceb8fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006773471832275391 sec\n",
            "WARNING:absl:Compiling prim_fun (139675139955792 for args (ShapedArray(int32[16]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0038700103759765625 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBmdVcbMRHEx"
      },
      "source": [
        "Now we are goingh to pmap the function so instead of accepting a single partition of a as an input, it takes n array with partitions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T_tpu = get_T_tpu(model)\n",
        "T_tpu_jit = jax.pmap(T_tpu, in_axes = (0,None)).lower(a_partitions, v_init).compile()\n",
        "%time T_tpu_jit(a_partitions,v_init).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wET5GrxCRPKc",
        "outputId": "e16bf327-061b-410d-e9b1-2ad49845d53a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming T_tpu for pmap in 0.03030681610107422 sec\n",
            "WARNING:absl:Compiling T_tpu (139825198945008) for 8 devices with args [ShapedArray(int32[8,2]), ShapedArray(float32[16,2])]. (num_replicas=8 num_partitions=1)\n",
            "WARNING:absl:Finished XLA compilation of T_tpu in 0.05223798751831055 sec\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 14 ms, sys: 874 ¬µs, total: 14.9 ms\n",
            "Wall time: 8.32 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ShardedDeviceArray([[[-1.8662423 , -0.23726259],\n",
              "                     [-1.2739494 , -0.21338044]],\n",
              "\n",
              "                    [[-0.9399282 , -0.1932523 ],\n",
              "                     [-0.7300421 , -0.17610094]],\n",
              "\n",
              "                    [[-0.5881006 , -0.16134453],\n",
              "                     [-0.48685533, -0.14853972]],\n",
              "\n",
              "                    [[-0.41165787, -0.13734336],\n",
              "                     [-0.3540047 , -0.12748614]],\n",
              "\n",
              "                    [[-0.30865595, -0.11875407],\n",
              "                     [-0.2722252 , -0.11097524]],\n",
              "\n",
              "                    [[-0.24243681, -0.10401004],\n",
              "                     [-0.21771078, -0.09774413]],\n",
              "\n",
              "                    [[-0.19691947, -0.09208304],\n",
              "                     [-0.17923889, -0.08694804]],\n",
              "\n",
              "                    [[-0.1640543 , -0.08227319],\n",
              "                     [-0.1508989 , -0.07800273]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "T_tpu = get_update_fn_tpu(model)\n",
        "T_tpu_jit = jax.pmap(T_tpu, in_axes = (0,None)).lower(a_partitions, v_init).compile()\n",
        "%timeit -r 1 -n 10 T_tpu_jit(a_partitions,v_init).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2bt6XWsRiRD"
      },
      "source": [
        "We get a X2 increase in throughput vs the manually vectorized solution. "
      ],
      "metadata": {
        "id": "m2bt6XWsRiRD"
      }
    },
    "id": "T_Kd-64jzk2a",
    "outputId": "74ba08ff-b1c2-448f-ce71-ef55d51c36e5"
  },
  "outputs": [
    {
      "cell_type": "code",
      "source": [
        "def vfi_iterator(v_init=v_init, tol=1e-6, max_iter=1435):\n",
        "    error = tol + 1\n",
        "    i = 0\n",
        "    v = v_init\n",
        "    while error > tol and i < max_iter:\n",
        "    # while i < max_iter:\n",
        "        new_v = T_tpu_jit(a_partitions,v).reshape(a_size,y_size)\n",
        "        error = jnp.max(jnp.abs(new_v - v))\n",
        "        v = new_v\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "        i += 1\n",
        "\n",
        "    if i == max_iter:\n",
        "        print(f\"Warning: iteration hit upper bound {max_iter}.\")\n",
        "    else:\n",
        "        print(f\"\\nConverged at iteration {i}.\")\n",
        "    return v\n",
        "\n",
        "%time v = vfi_iterator()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6mw4oTZ2Kvj",
        "outputId": "71ba15a4-1ed3-46c9-93fd-6c9685824dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Size of grid': [131072, 2097152, 8388608, 33554432, 134217728], 'Manual Vectorization': [0.004014438000001519, 0.19788137290000804, 1.5700353982000024, 12.1127872346, 97.47866143050001], 'Automatic Vectorization': [0.004789096399997561, 0.29747357039999545, 2.3173598094999988, 20.663012926700002, 224.7583419914], 'TPU Parallelization': [0.007923651699991296, 0.30005611290000617, 2.331630996399997, 20.0357178878, 223.66937429979998]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-compiling the entire experiment (not just the update)"
      ],
      "metadata": {
        "id": "Utsxm-o9SUGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1wn2smYuQUN"
      },
      "outputs": [],
      "source": [
        "def get_learner_fn(model:dict, iterations):\n",
        "\n",
        "  def v_update(v: jnp.ndarray, error):\n",
        "    params = model[\"params\"]\n",
        "    grids = model[\"grids\"]\n",
        "    P = model[\"Trans_matrix\"]\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      # \n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:], P[y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = model[\"indices\"][\"a\"]\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_v = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    new_error = jnp.max(jnp.abs(new_v - v))\n",
        "    return new_v, new_error\n",
        "\n",
        "  def learner_fn(v_init):  # repeat many times to avoid going back to Python.\n",
        "    return jax.lax.scan(v_update, v_init, None, length = iterations)\n",
        "\n",
        "  return learner_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHqUnEGNuhxC",
        "outputId": "5a52788e-a698-4d4b-973a-68be15f1af1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming learner_fn for jit in 0.037129878997802734 sec\n",
            "WARNING:absl:Compiling learner_fn (140487308656176 for args (ShapedArray(float32[4112,256]),).\n",
            "WARNING:absl:Finished XLA compilation of learner_fn in 0.9671173095703125 sec\n"
          ]
        }
      ],
      "source": [
        "n_iters = 1435\n",
        "learner_fn = get_learner_fn(model, n_iters)\n",
        "jitted_learner_fn = jax.jit(learner_fn).lower(v_init).compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "Z4G1_YNQu1Gd",
        "outputId": "2f97738a-fe0c-4d94-d6b5-6c201e5fa19a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006163120269775391 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289207984 for args (ShapedArray(float32[1435]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.021751880645751953 sec\n",
            "WARNING:absl:Finished tracing + transforming divmod for jit in 0.005877256393432617 sec\n",
            "WARNING:absl:Compiling divmod (140487289263984 for args (ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of divmod in 0.03612828254699707 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00038170814514160156 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289264272 for args (ShapedArray(int32[]),).\n",
            "WARNING:absl:Finished XLA compilation of convert_element_type in 0.01756453514099121 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004987716674804688 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289263792 for args (ShapedArray(float32[1435]), ShapedArray(int32[])).\n",
            "WARNING:absl:Finished XLA compilation of dynamic_slice in 0.01903390884399414 sec\n",
            "WARNING:absl:Finished tracing + transforming _unstack for jit in 0.020872831344604492 sec\n",
            "WARNING:absl:Compiling _unstack (140487288808656 for args (ShapedArray(float32[100]),).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 15.7 s, sys: 1min 11s, total: 1min 27s\n",
            "Wall time: 1min 24s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished XLA compilation of _unstack in 0.11274552345275879 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004360675811767578 sec\n",
            "WARNING:absl:Compiling prim_fun (140487288868688 for args (ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of lt in 0.02319025993347168 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004856586456298828 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289263984 for args (ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of add in 0.018721342086791992 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005552768707275391 sec\n",
            "WARNING:absl:Compiling prim_fun (140487288808656 for args (ShapedArray(bool[], weak_type=True), ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of select_n in 0.02396559715270996 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004684925079345703 sec\n",
            "WARNING:absl:Compiling prim_fun (140489049604176 for args (ShapedArray(int32[], weak_type=True),).\n",
            "WARNING:absl:Finished XLA compilation of convert_element_type in 0.02097344398498535 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006616115570068359 sec\n",
            "WARNING:absl:Compiling prim_fun (140487288808656 for args (ShapedArray(int32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.018973112106323242 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0007677078247070312 sec\n",
            "WARNING:absl:Compiling prim_fun (140489049604176 for args (ShapedArray(float32[1435,1]), ShapedArray(int32[1])).\n",
            "WARNING:absl:Finished XLA compilation of gather in 0.0232388973236084 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0007376670837402344 sec\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc5c0297d50>]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarks\n",
        "\n",
        "Now we are going to store our results for different levels of scales"
      ],
      "metadata": {
        "id": "FFAPDQ5I2m09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeIt():\n",
        "\n",
        "  def __init__(self, tag, frames=None):\n",
        "    self.tag = tag\n",
        "    self.frames = frames\n",
        "\n",
        "  def __enter__(self):\n",
        "    self.start = timeit.default_timer()\n",
        "    return self\n",
        "\n",
        "  def __exit__(self, *args):\n",
        "    self.elapsed_secs = timeit.default_timer() - self.start\n",
        "    msg = self.tag + (': Elapsed time=%.2fs' % self.elapsed_secs)\n",
        "    if self.frames:\n",
        "      msg += ', FPS=%.2e' % (self.frames / self.elapsed_secs)\n",
        "    print(msg)"
      ],
      "metadata": {
        "id": "3zP5W3fP5q7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_dict = {\"Size of grid\": [], \"Manual Vectorization\": [], \"Automatic Vectorization\": [], \"TPU Parallelization\": []}\n",
        "for scale in [1,2,4,8]:\n",
        "  # grid for assets\n",
        "  a_size = ap_size = 1024*scale\n",
        "  a_grid = jnp.linspace(a_min, a_max, a_size)  # grid for a\n",
        "  ap_grid = jnp.linspace(a_min, a_max, a_size)                 # grid for a'\n",
        "  #grid for y (use QuantEcon's tauchen() function to create Markov Chains out of AR(1))\n",
        "  œÅ = 0.9\n",
        "  œÉ = 0.1\n",
        "  y_size = 128*scale\n",
        "  mc = qe.tauchen(œÅ, œÉ, n=y_size)\n",
        "  y_grid = jnp.exp(mc.state_values)\n",
        "  P = jnp.array(mc.P)\n",
        "  results_dict[\"Size of grid\"].append(a_size*y_size)\n",
        "\n",
        "  grids = {\n",
        "    \"a\": a_grid,\n",
        "    \"y\": y_grid,\n",
        "    \"ap\": ap_grid,}\n",
        "\n",
        "  params = {\n",
        "      \"R\": 1.1,\n",
        "      \"beta\": 0.99,\n",
        "      \"gamma\": 2.5\n",
        "  }\n",
        "\n",
        "  model = {\"params\": params, \n",
        "          \"grids\": grids, \n",
        "          \"Trans_matrix\": P, \n",
        "          \"indices\": {\"a\": jnp.array(range(a_size)), \"y\": jnp.array(range(y_size)), \"ap\": jnp.array(range(ap_size))}}\n",
        "\n",
        "\n",
        "  # initial value\n",
        "  v_init = jnp.zeros((a_size, y_size))\n",
        "  P = jnp.reshape(P, (y_size, y_size, 1))\n",
        "  a = jnp.reshape(a_grid, (a_size, 1, 1))\n",
        "  y = jnp.reshape(y_grid, (1, y_size, 1))\n",
        "  ap = jnp.reshape(ap_grid, (1, 1, ap_size))\n",
        "\n",
        "\n",
        "  T_manualvec_jit = jax.jit(T_manualvec).lower(v_init).compile()\n",
        "  T_autovec = get_update_fn_autovec(model)\n",
        "  T_autovec_jit = jax.jit(T_autovec).lower(v_init).compile()\n",
        "\n",
        "  if use_TPU:\n",
        "    a_partitions = jnp.reshape(model[\"indices\"][\"a\"], (n_devices, a_size//n_devices))\n",
        "    T_tpu = get_update_fn_tpu(model)\n",
        "    T_tpu_jit = jax.pmap(T_tpu, in_axes = (0,None)).lower(a_partitions, v_init).compile()\n",
        "    results_dict[\"Manual Vectorization\"].append(timeit.timeit('T_manualvec_jit(v_init).block_until_ready()', globals=globals(), number=100))\n",
        "    results_dict[\"Automatic Vectorization\"].append(timeit.timeit('T_autovec_jit(v_init).block_until_ready()', globals=globals(), number=100))\n",
        "    results_dict[\"TPU Parallelization\"].append(timeit.timeit('T_tpu_jit(a_partitions, v_init).block_until_ready()', globals=globals(), number=100))\n",
        "  else:\n",
        "    results_dict[\"Manual Vectorization\"].append(timeit.timeit(\"T_manualvec_jit(v_init).block_until_ready()\", globals=globals(), number=100))\n",
        "    results_dict[\"Automatic Vectorization\"].append(timeit.timeit('T_autovec_jit(v_init).block_until_ready()', globals=globals(), number=100))\n",
        "\n",
        "print(results_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6mw4oTZ2Kvj",
        "outputId": "8742c76c-6b1b-4391-a783-7f784e19cb7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.006680011749267578 sec\n",
            "WARNING:absl:Compiling T_manualvec (140428908119920 for args (ShapedArray(float32[1024,128]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.12283062934875488 sec\n",
            "WARNING:absl:Finished tracing + transforming v_update for jit in 0.03142142295837402 sec\n",
            "WARNING:absl:Compiling v_update (140428908121648 for args (ShapedArray(float32[1024,128]),).\n",
            "WARNING:absl:Finished XLA compilation of v_update in 0.14154696464538574 sec\n",
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.005666017532348633 sec\n",
            "WARNING:absl:Compiling _linspace (140447419065456 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.06623101234436035 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.0004596710205078125 sec\n",
            "WARNING:absl:Compiling <lambda> (140428908120304 for args (ShapedArray(float32[256]),).\n",
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.09833669662475586 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.000396728515625 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00025010108947753906 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00021600723266601562 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0002815723419189453 sec\n",
            "WARNING:absl:Compiling prim_fun (140445169259824 for args (ShapedArray(float32[]),).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it works\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.00827646255493164 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003523826599121094 sec\n",
            "WARNING:absl:Compiling prim_fun (140445169259824 for args (ShapedArray(float32[256,256]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0031282901763916016 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.000308990478515625 sec\n",
            "WARNING:absl:Compiling prim_fun (140445169259824 for args (ShapedArray(float32[2048]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0025887489318847656 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004668235778808594 sec\n",
            "WARNING:absl:Compiling prim_fun (140428925179376 for args (ShapedArray(float32[256]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0027790069580078125 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004112720489501953 sec\n",
            "WARNING:absl:Compiling prim_fun (140428925178224 for args (ShapedArray(float32[2048]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.002217531204223633 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.006204843521118164 sec\n",
            "WARNING:absl:Compiling T_manualvec (140428925179376 for args (ShapedArray(float32[2048,256]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.22312068939208984 sec\n",
            "WARNING:absl:Finished tracing + transforming v_update for jit in 0.03150033950805664 sec\n",
            "WARNING:absl:Compiling v_update (140428907862928 for args (ShapedArray(float32[2048,256]),).\n",
            "WARNING:absl:Finished XLA compilation of v_update in 0.2447986602783203 sec\n",
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.005988597869873047 sec\n",
            "WARNING:absl:Compiling _linspace (140445903003088 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.06551384925842285 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0002856254577636719 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00030112266540527344 sec\n",
            "WARNING:absl:Compiling prim_fun (140445907610800 for args (ShapedArray(float32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.06955218315124512 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003516674041748047 sec\n",
            "WARNING:absl:Compiling prim_fun (140445907610800 for args (ShapedArray(float32[4096]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003458261489868164 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003108978271484375 sec\n",
            "WARNING:absl:Compiling prim_fun (140445907610800 for args (ShapedArray(float32[4096]),).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it works\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished XLA compilation of reshape in 0.0031731128692626953 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.006471395492553711 sec\n",
            "WARNING:absl:Compiling T_manualvec (140445907610800 for args (ShapedArray(float32[4096,512]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.25492048263549805 sec\n",
            "WARNING:absl:Finished tracing + transforming v_update for jit in 0.026103496551513672 sec\n",
            "WARNING:absl:Compiling v_update (140428908119920 for args (ShapedArray(float32[4096,512]),).\n",
            "WARNING:absl:Finished XLA compilation of v_update in 0.27561426162719727 sec\n",
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.006505489349365234 sec\n",
            "WARNING:absl:Compiling _linspace (140428925305872 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.06710243225097656 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.0004940032958984375 sec\n",
            "WARNING:absl:Compiling <lambda> (140445903003088 for args (ShapedArray(float32[1024]),).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it works\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.10049295425415039 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.000308990478515625 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003218650817871094 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003070831298828125 sec\n",
            "WARNING:absl:Compiling prim_fun (140428925178320 for args (ShapedArray(float32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.06606817245483398 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004177093505859375 sec\n",
            "WARNING:absl:Compiling prim_fun (140428925178320 for args (ShapedArray(float32[1024,1024]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.002988100051879883 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003352165222167969 sec\n",
            "WARNING:absl:Compiling prim_fun (140428907863888 for args (ShapedArray(float32[8192]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0029296875 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00036263465881347656 sec\n",
            "WARNING:absl:Compiling prim_fun (140428907863888 for args (ShapedArray(float32[1024]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0026056766510009766 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00038361549377441406 sec\n",
            "WARNING:absl:Compiling prim_fun (140428907989808 for args (ShapedArray(float32[8192]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.00241851806640625 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.0060787200927734375 sec\n",
            "WARNING:absl:Compiling T_manualvec (140428907987120 for args (ShapedArray(float32[8192,1024]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.40097594261169434 sec\n",
            "WARNING:absl:Finished tracing + transforming v_update for jit in 0.02709054946899414 sec\n",
            "WARNING:absl:Compiling v_update (140428907888720 for args (ShapedArray(float32[8192,1024]),).\n",
            "WARNING:absl:Finished XLA compilation of v_update in 0.4282653331756592 sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it works\n",
            "{'Size of grid': [131072, 524288, 2097152, 8388608], 'Manual Vectorization': [0.37764132200004497, 2.5571336220000376, 19.766007327999887, 157.03423161800015], 'Automatic Vectorization': [0.4339618720000544, 3.02273190599999, 31.13093618799985, 237.56771815899992], 'TPU Parallelization': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " For TPU, we get {'Manual Vectorization': [0.30772871099907206, 1.4740470310025557, 9.87848552899959, 77.9623624650012], 'Automatic Vectorization': [0.3273367039982986, 1.5696243140009756, 12.601403237997147, 92.91537422500187], 'TPU Parallelization': [0.8347508730003028, 1.4450628260019585, 3.4721897010022076, 17.260116208999534]}\n",
        "\n",
        " For GPU{'Size of grid': [131072, 524288, 2097152, 8388608], 'Manual Vectorization': [0.37764132200004497, 2.5571336220000376, 19.766007327999887, 157.03423161800015], 'Automatic Vectorization': [0.4339618720000544, 3.02273190599999, 31.13093618799985, 237.56771815899992]}"
      ],
      "metadata": {
        "id": "d56TPREHHvC3"
      }
    }
  ]
}
