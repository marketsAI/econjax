{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasCovarrubias/econjax/blob/main/VFI_using_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hDVZWighqhA"
      },
      "source": [
        "# Parallelized Value Function Iteration Using JAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRO5lTwrhyCp"
      },
      "source": [
        "This colab notebook explores ideas on how to optimally use JAX to perdorm value funciton iteration. The objective is to improve on https://notes.quantecon.org/submission/622ed4daf57192000f918c61 . Hypothetically, we can get better performance by using vmap, making all the fuctions pure, using lax.scan for iterations and using the most optimal jax functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuQOCHhqiXl5"
      },
      "source": [
        "The problem is to maximize the expected discounted sum\n",
        "\n",
        "$$ ð”¼\\sum_{tâ‰¥0} Î²^t u(c_t)$$\n",
        "\n",
        "subject to\n",
        "\n",
        "$$c_t+a_t+1â‰¤Ra_t+yt, \\quad c_tâ‰¥0,\\quad a_tâ‰¥0$$\n",
        "\n",
        "for all $tâ‰¥0$, with $a_0$ and $y_0$ given. Here $c_t$ is consumption, $a_t$ is assets, $R$ is the gross risk-free rate of return, and $y_t$ is income. The income process follows a Markov chain with transition matrix $P$.\n",
        "\n",
        "The Bellman equation is\n",
        "\n",
        "$$v(a,y)= \\underset{0â‰¤aâ€²â‰¤Ra+y}{\\max} \\{ u(Ra+yâˆ’aâ€²)+Î²\\sum_{yâ€²}v(aâ€²,yâ€²)P(y,yâ€²) \\}$$\n",
        "\n",
        "where $v$ is the value function. The corresponding Bellman operator is\n",
        "\n",
        "$$Tv(a,y)= \\underset{0â‰¤aâ€²â‰¤Ra+y}{\\max} \\{ u(Ra+yâˆ’aâ€²)+Î²\\sum_{yâ€²}v(aâ€²,yâ€²)P(y,yâ€²) \\}$$\n",
        "\n",
        "We solve the dynamic program by value function iteration --- that is, by iterating with T.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VGwrYCDnece"
      },
      "source": [
        "We will start by installing quantecon and importing the libraries we will use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQqkG9rlkPy3",
        "outputId": "79af5a18-6329-46fe-bf20-32a85b4b4753"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting quantecon\n",
            "  Downloading quantecon-0.5.3-py3-none-any.whl (179 kB)\n",
            "\u001b[?25l\r\u001b[K     |â–ˆâ–‰                              | 10 kB 37.8 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–‹                            | 20 kB 34.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 30 kB 20.6 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 40 kB 17.1 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 51 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 61 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 71 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 81 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 92 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 102 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 112 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 122 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 133 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 143 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 153 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 163 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 174 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 179 kB 16.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from quantecon) (0.51.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from quantecon) (2.23.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.7.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from quantecon) (1.21.6)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->quantecon) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->quantecon) (57.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->quantecon) (1.24.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->quantecon) (1.2.1)\n",
            "Installing collected packages: quantecon\n",
            "Successfully installed quantecon-0.5.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        }
      ],
      "source": [
        "# installs\n",
        "!pip install -U quantecon # Install quantecon in case it's missing\n",
        "\n",
        "#imports\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from numba import njit\n",
        "import quantecon as qe \n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "from jax.config import config\n",
        "config.update(\"jax_log_compiles\", 1)\n",
        "# to suppress watnings uncomment next two lines\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijfXQhUfIyqG"
      },
      "source": [
        "# Starting Point: Naive implementation on a small grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2Joo0ilkTRc"
      },
      "source": [
        "Next, we specify global parameters (we will then move it to local), the grids for the state and actions, and the inital value for the value function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz8O9k67kZxq",
        "outputId": "089ceace-4677-4796-ce00-b96959cd3484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "state size: 1024\n"
          ]
        }
      ],
      "source": [
        "R = 1.1\n",
        "Î² = 0.99\n",
        "Î³ = 2.5\n",
        "\n",
        "# grid for assets\n",
        "a_min, a_max = 0.01, 2\n",
        "a_size = ap_size = 128\n",
        "a_grid = np.linspace(a_min, a_max, a_size)  # grid for a\n",
        "ap_grid = np.linspace(a_min, a_max, a_size)                 # grid for a'\n",
        "\n",
        "#grid for y (use QuantEcon's tauchen() function to create Markov Chains out of AR(1))\n",
        "Ï = 0.9\n",
        "Ïƒ = 0.1\n",
        "y_size = 8\n",
        "mc = qe.tauchen(Ï, Ïƒ, n=y_size)\n",
        "y_grid = np.exp(mc.state_values)\n",
        "P = np.array(mc.P)\n",
        "print(\"state size:\", a_size*y_size)\n",
        "\n",
        "# initial value\n",
        "v_init = np.zeros((a_size, y_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sEZEyoRGinJ"
      },
      "source": [
        "We are going to start by using a nainve approach that uses for loops extensively. This structure is going t o be the base of our \"authomatically vectorized\" version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyjlPg9DGo7_",
        "outputId": "63007fc9-699c-4fb6-eb29-6cb0c7b11203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 457 ms, sys: 11.9 ms, total: 469 ms\n",
            "Wall time: 458 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[-1.86623555, -1.39026198, -1.03558477, ..., -0.42783137,\n",
              "        -0.3186092 , -0.2372622 ],\n",
              "       [-1.77434107, -1.33358887, -1.00068349, ..., -0.41973168,\n",
              "        -0.31363942, -0.23421456],\n",
              "       [-1.68974815, -1.28066418, -0.96769951, ..., -0.41188438,\n",
              "        -0.3087975 , -0.2312316 ],\n",
              "       ...,\n",
              "       [-0.15384391, -0.14482871, -0.13498711, ..., -0.10187474,\n",
              "        -0.09033787, -0.07897758],\n",
              "       [-0.1523595 , -0.143486  , -0.13379257, ..., -0.1011265 ,\n",
              "        -0.08972517, -0.0784876 ],\n",
              "       [-0.15089881, -0.14216391, -0.13261555, ..., -0.10038738,\n",
              "        -0.08911936, -0.07800266]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def u(c):\n",
        "    return c**(1-Î³) / (1-Î³)\n",
        "\n",
        "def T_naive(v):\n",
        "    \"The Bellman operator.\"\n",
        "    # Allocate memory\n",
        "    v_new = np.empty_like(v)\n",
        "    # Step through all states\n",
        "    for i, a in enumerate(a_grid):\n",
        "        for j, y in enumerate(y_grid):\n",
        "            # Choose a' optimally by stepping through all possible values\n",
        "            v_max = - np.inf\n",
        "            for k, ap in enumerate(ap_grid):\n",
        "                c = R * a + y - ap\n",
        "                if c > 0:  \n",
        "                    # Calculate the right hand side of the Belllman operator\n",
        "                    val = u(c) + Î² * np.dot(v[k, :], P[j, :])\n",
        "                    if val > v_max:\n",
        "                        v_max = val\n",
        "            v_new[i, j] = v_max\n",
        "    return v_new\n",
        "\n",
        "%time T_naive(v_init)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgoIy7ARltZK"
      },
      "source": [
        "# Second pass: vectorized solution on large grid using JAX and accelerators\n",
        "\n",
        " First, let's check that Google Colab has assigned us a nice GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q4rf7AnoPr4"
      },
      "source": [
        "If we want to use TPUs, set use_TPU = True in the next cel. If not, the code !nvidia-smi will tell us what GPU has been asigned to us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O8trXkroSxM",
        "outputId": "5f4956c4-649f-4c77-a125-a80d8786e230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu May 12 02:38:38 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "use_TPU = False\n",
        "if use_TPU:\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "  !nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzQ8Vxo3mBbc"
      },
      "source": [
        "In the case you chose a GPU runtime, if you don't use Colab Pro, you should get something lika a Tesla K80. Since I am using Colab Pro, I got a Tesla P100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm50me2WKYpY"
      },
      "source": [
        "Now we are going to redefine de grids using jax.numpy arrays and we are going to increase the size of the grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSn2pgvBKN-S",
        "outputId": "92acbf26-37e4-4de6-b5bc-021a60b1a498"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.009136199951171875 sec\n",
            "WARNING:absl:Compiling _linspace (140445892217168 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.4287600517272949 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.00047588348388671875 sec\n",
            "WARNING:absl:Compiling <lambda> (140445892217456 for args (ShapedArray(float32[512]),).\n",
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.10872650146484375 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003161430358886719 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00029659271240234375 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00038361549377441406 sec\n",
            "WARNING:absl:Compiling prim_fun (140445904128816 for args (ShapedArray(float32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.0665590763092041 sec\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "state size: 2105344\n"
          ]
        }
      ],
      "source": [
        "# grid for assets\n",
        "scale=4\n",
        "a_size = ap_size = 1028*scale\n",
        "a_grid = jnp.linspace(a_min, a_max, a_size)  # grid for a\n",
        "ap_grid = jnp.linspace(a_min, a_max, a_size)                 # grid for a'\n",
        "\n",
        "#grid for y (use QuantEcon's tauchen() function to create Markov Chains out of AR(1))\n",
        "Ï = 0.9\n",
        "Ïƒ = 0.1\n",
        "y_size = 128*scale\n",
        "mc = qe.tauchen(Ï, Ïƒ, n=y_size)\n",
        "y_grid = jnp.exp(mc.state_values)\n",
        "P = jnp.array(mc.P)\n",
        "print(\"state size:\", a_size*y_size)\n",
        "\n",
        "# initial value\n",
        "v_init = jnp.zeros((a_size, y_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W6SRD3FmRD_"
      },
      "source": [
        "## Manual vectorization: Reshape grids and transition matrix\n",
        "\n",
        "\n",
        "We add dimensions to arrays so that they will be stretched along the new dimensions when placed in arithmetic operations with other arrays that have more elements along those dimensions. This stretching is done by repeating values, which is what we use to replace loops.\n",
        "\n",
        "The next code cell reshapes all arrays to be three-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkjoN5cfmm0Y",
        "outputId": "d329e091-9ec8-4239-d12f-b4327bf53abe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00041961669921875 sec\n",
            "WARNING:absl:Compiling prim_fun (140445891848528 for args (ShapedArray(float32[512,512]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0034322738647460938 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00038313865661621094 sec\n",
            "WARNING:absl:Compiling prim_fun (140445904127472 for args (ShapedArray(float32[4112]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.002595186233520508 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003726482391357422 sec\n",
            "WARNING:absl:Compiling prim_fun (140445904127472 for args (ShapedArray(float32[512]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0024871826171875 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00031375885009765625 sec\n",
            "WARNING:absl:Compiling prim_fun (140445904149392 for args (ShapedArray(float32[4112]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0022416114807128906 sec\n"
          ]
        }
      ],
      "source": [
        "P = jnp.reshape(P, (y_size, y_size, 1))\n",
        "a = jnp.reshape(a_grid, (a_size, 1, 1))\n",
        "y = jnp.reshape(y_grid, (1, y_size, 1))\n",
        "ap = jnp.reshape(ap_grid, (1, 1, ap_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJBjSuCQm4MY"
      },
      "source": [
        "Now we can implement a vectorized version of the Bellman operator, which calculates the same values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMVrgeNim435"
      },
      "outputs": [],
      "source": [
        "def u(c):\n",
        "    return c**(1-Î³) / (1-Î³)\n",
        "\n",
        "def T_manualvec(v):\n",
        "    vp = jnp.dot(v, P) # vp has shape (a_size, y_size, 1)\n",
        "    c = R * a + y - ap # c has shape (a_size, y_size, ap_size)\n",
        "    # m = jnp.where(c > 0, u(c) + Î² * vp, -np.inf) # m has shape (a_size, y_size, ap_size) \n",
        "    m = u(c) + Î² * vp # m has shape (a_size, y_size, ap_size) \n",
        "    return jnp.max(m, axis=2) # we to average over the last axis, that is, for each a and y, get the the max over ap.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4spbHVTm___"
      },
      "source": [
        "Now wer are gonig to precompile the T operation so JAX optimize it use of the hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeDrpKwbtsgQ",
        "outputId": "95ab225c-b450-4024-f6da-04f459ca1d8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.007188558578491211 sec\n",
            "WARNING:absl:Compiling T_manualvec (140445908120688 for args (ShapedArray(float32[4112,512]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.14515233039855957 sec\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 208 ms, sys: 0 ns, total: 208 ms\n",
            "Wall time: 385 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeviceArray([[        nan,         nan,         nan, ...,         nan,\n",
              "                      nan, -0.23726219],\n",
              "             [        nan,         nan,         nan, ...,         nan,\n",
              "                      nan, -0.23716709],\n",
              "             [        nan,         nan,         nan, ...,         nan,\n",
              "                      nan, -0.23707198],\n",
              "             ...,\n",
              "             [-0.15098839, -0.1508744 , -0.15076025, ..., -0.07833294,\n",
              "              -0.07818268, -0.07803249],\n",
              "             [-0.15094359, -0.15082967, -0.15071559, ..., -0.07831794,\n",
              "              -0.07816772, -0.07801757],\n",
              "             [-0.15089881, -0.15078494, -0.15067092, ..., -0.07830293,\n",
              "              -0.07815276, -0.07800266]], dtype=float32)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "T_manualvec_jit = jax.jit(T_manualvec).lower(v_init).compile()\n",
        "%time T_manualvec_jit(v_init).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iNU6avAozFx"
      },
      "source": [
        "With a Tesla P100 GPU, we get 30ms. With TPU, we get 16.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTHLO-DHMv1m"
      },
      "source": [
        "## Authomatic vecotrization using vmap\n",
        "\n",
        "To start, we are going to save all the parts we need in a model dictionary to have pure functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M2L0rxf6shO",
        "outputId": "762e69fc-714c-431a-a909-0e3480f73048"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00030350685119628906 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00042891502380371094 sec\n"
          ]
        }
      ],
      "source": [
        "# We are going to store the structure of the model in a dictionary\n",
        "grids = {\n",
        "    \"a\": a_grid,\n",
        "    \"y\": y_grid,\n",
        "    \"ap\": ap_grid,}\n",
        "\n",
        "params = {\n",
        "    \"R\": 1.1,\n",
        "    \"beta\": 0.99,\n",
        "    \"gamma\": 2.5\n",
        "}\n",
        "\n",
        "model = {\"params\": params, \n",
        "         \"grids\": grids, \n",
        "         \"Trans_matrix\": P, \n",
        "         \"indices\": {\"a\": jnp.array(range(a_size)), \"y\": jnp.array(range(y_size)), \"ap\": jnp.array(range(ap_size))}}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn5sngld7KX9"
      },
      "source": [
        "When we calculate the value function at each point of the grid, we want to pass all the possible actions as a vector, and then calculate the maximum. In order to get the actio_value for all the possible actions, we are going to use vmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3zJaJk89npP"
      },
      "outputs": [],
      "source": [
        "def get_update_fn_autovec(model:dict):\n",
        "\n",
        "  def v_update(v: jnp.ndarray):\n",
        "    params = model[\"params\"]\n",
        "    grids = model[\"grids\"]\n",
        "    P = model[\"Trans_matrix\"]\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      # \n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:],P[y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = model[\"indices\"][\"a\"]\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_state_value = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    return new_state_value\n",
        "\n",
        "  return v_update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27x1EKcp95xh",
        "outputId": "a8feec55-a8c7-4058-8911-541c4f858565"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming v_update for jit in 0.031070470809936523 sec\n",
            "WARNING:absl:Compiling v_update (140445907181648 for args (ShapedArray(float32[4112,512]),).\n",
            "WARNING:absl:Finished XLA compilation of v_update in 0.2908954620361328 sec\n"
          ]
        }
      ],
      "source": [
        "T_autovec = get_update_fn_autovec(model)\n",
        "T_autovec_jit = jax.jit(T_autovec).lower(v_init).compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks6iS0OVONFR",
        "outputId": "e7bb1c23-a05f-4ce3-c3bb-70f577ac6260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3.85 ms, sys: 867 Âµs, total: 4.71 ms\n",
            "Wall time: 431 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeviceArray([[-1.8662355 , -1.858725  , -1.8512443 , ..., -0.23918638,\n",
              "              -0.23822235, -0.23726219],\n",
              "             [-1.8632786 , -1.8557878 , -1.848327  , ..., -0.23908995,\n",
              "              -0.23812656, -0.23716709],\n",
              "             [-1.8603299 , -1.8528588 , -1.8454175 , ..., -0.23899359,\n",
              "              -0.23803085, -0.23707198],\n",
              "             ...,\n",
              "             [-0.15098839, -0.1508744 , -0.15076025, ..., -0.07833294,\n",
              "              -0.07818268, -0.07803249],\n",
              "             [-0.15094359, -0.15082967, -0.15071559, ..., -0.07831794,\n",
              "              -0.07816772, -0.07801757],\n",
              "             [-0.15089881, -0.15078494, -0.15067092, ..., -0.07830293,\n",
              "              -0.07815276, -0.07800266]], dtype=float32)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%time T_autovec_jit(v_init).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyMHJQ4OpAfB"
      },
      "source": [
        "To finish the exercise off, let's iterate until convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqgYaYNFpDNH",
        "outputId": "688b9242-69b4-4a8a-9276-c6ec942e752e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0\n",
            "Iteration 100\n",
            "Iteration 200\n",
            "Iteration 300\n",
            "Iteration 400\n",
            "Iteration 500\n",
            "Iteration 600\n",
            "Iteration 700\n",
            "Iteration 800\n",
            "Iteration 900\n",
            "Iteration 1000\n",
            "Iteration 1100\n",
            "Iteration 1200\n",
            "Iteration 1300\n",
            "Iteration 1400\n",
            "Warning: iteration hit upper bound 1435.\n",
            "CPU times: user 17.3 s, sys: 1min 18s, total: 1min 35s\n",
            "Wall time: 1min 33s\n"
          ]
        }
      ],
      "source": [
        "def vfi_iterator(v_init=v_init, tol=1e-6, max_iter=1435):\n",
        "    error = tol + 1\n",
        "    i = 0\n",
        "    v = v_init\n",
        "    # while error > tol and i < max_iter:\n",
        "    while i < max_iter:\n",
        "        new_v = T_autovec_jit(v)\n",
        "        error = jnp.max(jnp.abs(new_v - v))\n",
        "        v = new_v\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "        i += 1\n",
        "\n",
        "    if i == max_iter:\n",
        "        print(f\"Warning: iteration hit upper bound {max_iter}.\")\n",
        "    else:\n",
        "        print(f\"\\nConverged at iteration {i}.\")\n",
        "    return v\n",
        "\n",
        "%time v = vfi_iterator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2_IS1SzOinP"
      },
      "source": [
        "## Parallelizing across TPU cores using pmaps\n",
        "\n",
        "We are going to make a very small change to our update funciton so we can give a partition of the grid of a as an input and we get can update the value function only on those states. Then, with pmap, we will vectorize that function so we give a n array with n_device partitions of a and it will update those partitions of the state in parallel across TPU_cores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px-H46ldOnOQ"
      },
      "outputs": [],
      "source": [
        "def get_update_fn_tpu(model:dict):\n",
        "\n",
        "  def v_update(a_partition:jnp.array, v: jnp.ndarray):\n",
        "    params = model[\"params\"]\n",
        "    grids = model[\"grids\"]\n",
        "    P = model[\"Trans_matrix\"]\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      # \n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:], model[\"Trans_matrix\"][y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = a_partition\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_state_value = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    return new_state_value\n",
        "\n",
        "  return v_update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVlCCZV0PaYT"
      },
      "source": [
        "No, we create an array with the partitions. The dimension of that array should be (n_cores,a_size/n_cores), so the leading axis organize the different partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rky2k2rIPZh1"
      },
      "outputs": [],
      "source": [
        "n_devices = jax.local_device_count()\n",
        "a_partitions = jnp.reshape(model[\"indices\"][\"a\"], (n_devices, a_size//n_devices))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBmdVcbMRHEx"
      },
      "source": [
        "Now we are goingh to pmap the function so instead of accepting a single partition of a as an input, it takes n array with partitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wET5GrxCRPKc",
        "outputId": "704b3fb8-93ab-4467-c970-fea8534faaec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming v_update for pmap in 0.02937602996826172 sec\n",
            "WARNING:absl:Compiling v_update (140512946252976) for 8 devices with args [ShapedArray(int32[8,128]), ShapedArray(float32[1024,128])]. (num_replicas=8 num_partitions=1)\n",
            "WARNING:absl:Finished XLA compilation of v_update in 0.4121882915496826 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.002092123031616211 sec\n",
            "WARNING:absl:Compiling _multi_slice (140512946191824 for args (ShapedArray(int32[8,128]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.03167271614074707 sec\n",
            "WARNING:absl:Finished tracing + transforming _multi_slice for jit in 0.0014331340789794922 sec\n",
            "WARNING:absl:Compiling _multi_slice (140513164227504 for args (ShapedArray(float32[1024,128]),).\n",
            "WARNING:absl:Finished XLA compilation of _multi_slice in 0.03240251541137695 sec\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 loops, best of 1: 18.5 ms per loop\n"
          ]
        }
      ],
      "source": [
        "T_tpu = get_update_fn_tpu(model)\n",
        "T_tpu_jit = jax.pmap(T_tpu, in_axes = (0,None)).lower(a_partitions, v_init).compile()\n",
        "%timeit -r 1 -n 10 T_tpu_jit(a_partitions,v_init).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2bt6XWsRiRD"
      },
      "source": [
        "We get a X2 increase in throughput vs the manually vectorized solution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_Kd-64jzk2a",
        "outputId": "74ba08ff-b1c2-448f-ce71-ef55d51c36e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0\n",
            "Iteration 100\n",
            "Iteration 200\n",
            "Iteration 300\n",
            "Iteration 400\n",
            "Iteration 500\n",
            "Iteration 600\n",
            "\n",
            "Converged at iteration 601.\n",
            "CPU times: user 27.6 s, sys: 39.4 s, total: 1min 6s\n",
            "Wall time: 48 s\n"
          ]
        }
      ],
      "source": [
        "def vfi_iterator(v_init=v_init, tol=1e-6, max_iter=1435):\n",
        "    error = tol + 1\n",
        "    i = 0\n",
        "    v = v_init\n",
        "    while error > tol and i < max_iter:\n",
        "    # while i < max_iter:\n",
        "        new_v = T_tpu_jit(a_partitions,v).reshape(a_size,y_size)\n",
        "        error = jnp.max(jnp.abs(new_v - v))\n",
        "        v = new_v\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}\")\n",
        "        i += 1\n",
        "\n",
        "    if i == max_iter:\n",
        "        print(f\"Warning: iteration hit upper bound {max_iter}.\")\n",
        "    else:\n",
        "        print(f\"\\nConverged at iteration {i}.\")\n",
        "    return v\n",
        "\n",
        "%time v = vfi_iterator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utsxm-o9SUGI"
      },
      "source": [
        "# Pre-compiling the entire experiment (not just the update)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1wn2smYuQUN"
      },
      "outputs": [],
      "source": [
        "def get_learner_fn(model:dict, iterations):\n",
        "\n",
        "  def v_update(v: jnp.ndarray, error):\n",
        "    params = model[\"params\"]\n",
        "    grids = model[\"grids\"]\n",
        "    P = model[\"Trans_matrix\"]\n",
        "    def action_v(a_ind: int, y_ind: int, ap_ind: int, v: jnp.array):\n",
        "      c = params[\"R\"]*grids[\"a\"][a_ind]+grids[\"y\"][y_ind]-grids[\"ap\"][ap_ind]\n",
        "      # \n",
        "      return jnp.where(c>0, c**(1-params[\"gamma\"]) / (1-params[\"gamma\"]) + params[\"beta\"] * jnp.dot(v[ap_ind,:], P[y_ind, :]) ,- jnp.inf)\n",
        "    # first vmap to calculate action value for all possible ap's.\n",
        "    vmapped_action_v = jax.vmap(action_v, in_axes=(None,None,0, None))\n",
        "    # get the maximum of all action_values for a pair of (a,y)\n",
        "    def one_state_v(a_ind: int, y_ind: int, ap_grid: jnp.array, v: jnp.array):\n",
        "      return jnp.max(vmapped_action_v(a_ind, y_ind, ap_grid, v))\n",
        "\n",
        "    # do vmaps over the other two dimensions.\n",
        "    all_state_v = jax.vmap(jax.vmap(one_state_v, in_axes=(None,0,None, None)), in_axes=(0,None,None, None))\n",
        "    #calculate value fuction matrix\n",
        "    a_indices = model[\"indices\"][\"a\"]\n",
        "    y_indices = model[\"indices\"][\"y\"]\n",
        "    ap_indices = model[\"indices\"][\"ap\"]\n",
        "    new_v = all_state_v(a_indices,y_indices,ap_indices, v)\n",
        "    new_error = jnp.max(jnp.abs(new_v - v))\n",
        "    return new_v, new_error\n",
        "\n",
        "  def learner_fn(v_init):  # repeat many times to avoid going back to Python.\n",
        "    return jax.lax.scan(v_update, v_init, None, length = iterations)\n",
        "\n",
        "  return learner_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHqUnEGNuhxC",
        "outputId": "5a52788e-a698-4d4b-973a-68be15f1af1d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming learner_fn for jit in 0.037129878997802734 sec\n",
            "WARNING:absl:Compiling learner_fn (140487308656176 for args (ShapedArray(float32[4112,256]),).\n",
            "WARNING:absl:Finished XLA compilation of learner_fn in 0.9671173095703125 sec\n"
          ]
        }
      ],
      "source": [
        "n_iters = 1435\n",
        "learner_fn = get_learner_fn(model, n_iters)\n",
        "jitted_learner_fn = jax.jit(learner_fn).lower(v_init).compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "Z4G1_YNQu1Gd",
        "outputId": "2f97738a-fe0c-4d94-d6b5-6c201e5fa19a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006163120269775391 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289207984 for args (ShapedArray(float32[1435]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.021751880645751953 sec\n",
            "WARNING:absl:Finished tracing + transforming divmod for jit in 0.005877256393432617 sec\n",
            "WARNING:absl:Compiling divmod (140487289263984 for args (ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of divmod in 0.03612828254699707 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00038170814514160156 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289264272 for args (ShapedArray(int32[]),).\n",
            "WARNING:absl:Finished XLA compilation of convert_element_type in 0.01756453514099121 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004987716674804688 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289263792 for args (ShapedArray(float32[1435]), ShapedArray(int32[])).\n",
            "WARNING:absl:Finished XLA compilation of dynamic_slice in 0.01903390884399414 sec\n",
            "WARNING:absl:Finished tracing + transforming _unstack for jit in 0.020872831344604492 sec\n",
            "WARNING:absl:Compiling _unstack (140487288808656 for args (ShapedArray(float32[100]),).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 15.7 s, sys: 1min 11s, total: 1min 27s\n",
            "Wall time: 1min 24s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished XLA compilation of _unstack in 0.11274552345275879 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004360675811767578 sec\n",
            "WARNING:absl:Compiling prim_fun (140487288868688 for args (ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of lt in 0.02319025993347168 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004856586456298828 sec\n",
            "WARNING:absl:Compiling prim_fun (140487289263984 for args (ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of add in 0.018721342086791992 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0005552768707275391 sec\n",
            "WARNING:absl:Compiling prim_fun (140487288808656 for args (ShapedArray(bool[], weak_type=True), ShapedArray(int32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of select_n in 0.02396559715270996 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004684925079345703 sec\n",
            "WARNING:absl:Compiling prim_fun (140489049604176 for args (ShapedArray(int32[], weak_type=True),).\n",
            "WARNING:absl:Finished XLA compilation of convert_element_type in 0.02097344398498535 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0006616115570068359 sec\n",
            "WARNING:absl:Compiling prim_fun (140487288808656 for args (ShapedArray(int32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.018973112106323242 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0007677078247070312 sec\n",
            "WARNING:absl:Compiling prim_fun (140489049604176 for args (ShapedArray(float32[1435,1]), ShapedArray(int32[1])).\n",
            "WARNING:absl:Finished XLA compilation of gather in 0.0232388973236084 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0007376670837402344 sec\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc5c0297d50>]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%time value_final, error_stack = jitted_learner_fn(v_init)\n",
        "plt.plot(range(n_iters),error_stack)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFAPDQ5I2m09"
      },
      "source": [
        "# Benchmarks\n",
        "\n",
        "Now we are going to store our results for different levels of scales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zP5W3fP5q7s"
      },
      "outputs": [],
      "source": [
        "class TimeIt():\n",
        "\n",
        "  def __init__(self, tag, frames=None):\n",
        "    self.tag = tag\n",
        "    self.frames = frames\n",
        "\n",
        "  def __enter__(self):\n",
        "    self.start = timeit.default_timer()\n",
        "    return self\n",
        "\n",
        "  def __exit__(self, *args):\n",
        "    self.elapsed_secs = timeit.default_timer() - self.start\n",
        "    msg = self.tag + (': Elapsed time=%.2fs' % self.elapsed_secs)\n",
        "    if self.frames:\n",
        "      msg += ', FPS=%.2e' % (self.frames / self.elapsed_secs)\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6mw4oTZ2Kvj",
        "outputId": "8742c76c-6b1b-4391-a783-7f784e19cb7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.006680011749267578 sec\n",
            "WARNING:absl:Compiling T_manualvec (140428908119920 for args (ShapedArray(float32[1024,128]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.12283062934875488 sec\n",
            "WARNING:absl:Finished tracing + transforming v_update for jit in 0.03142142295837402 sec\n",
            "WARNING:absl:Compiling v_update (140428908121648 for args (ShapedArray(float32[1024,128]),).\n",
            "WARNING:absl:Finished XLA compilation of v_update in 0.14154696464538574 sec\n",
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.005666017532348633 sec\n",
            "WARNING:absl:Compiling _linspace (140447419065456 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.06623101234436035 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.0004596710205078125 sec\n",
            "WARNING:absl:Compiling <lambda> (140428908120304 for args (ShapedArray(float32[256]),).\n",
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.09833669662475586 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.000396728515625 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00025010108947753906 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00021600723266601562 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0002815723419189453 sec\n",
            "WARNING:absl:Compiling prim_fun (140445169259824 for args (ShapedArray(float32[]),).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "it works\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.00827646255493164 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003523826599121094 sec\n",
            "WARNING:absl:Compiling prim_fun (140445169259824 for args (ShapedArray(float32[256,256]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0031282901763916016 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.000308990478515625 sec\n",
            "WARNING:absl:Compiling prim_fun (140445169259824 for args (ShapedArray(float32[2048]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0025887489318847656 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004668235778808594 sec\n",
            "WARNING:absl:Compiling prim_fun (140428925179376 for args (ShapedArray(float32[256]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0027790069580078125 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004112720489501953 sec\n",
            "WARNING:absl:Compiling prim_fun (140428925178224 for args (ShapedArray(float32[2048]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.002217531204223633 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.006204843521118164 sec\n",
            "WARNING:absl:Compiling T_manualvec (140428925179376 for args (ShapedArray(float32[2048,256]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.22312068939208984 sec\n",
            "WARNING:absl:Finished tracing + transforming v_update for jit in 0.03150033950805664 sec\n",
            "WARNING:absl:Compiling v_update (140428907862928 for args (ShapedArray(float32[2048,256]),).\n",
            "WARNING:absl:Finished XLA compilation of v_update in 0.2447986602783203 sec\n",
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.005988597869873047 sec\n",
            "WARNING:absl:Compiling _linspace (140445903003088 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.06551384925842285 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0002856254577636719 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00030112266540527344 sec\n",
            "WARNING:absl:Compiling prim_fun (140445907610800 for args (ShapedArray(float32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.06955218315124512 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003516674041748047 sec\n",
            "WARNING:absl:Compiling prim_fun (140445907610800 for args (ShapedArray(float32[4096]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.003458261489868164 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003108978271484375 sec\n",
            "WARNING:absl:Compiling prim_fun (140445907610800 for args (ShapedArray(float32[4096]),).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "it works\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished XLA compilation of reshape in 0.0031731128692626953 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.006471395492553711 sec\n",
            "WARNING:absl:Compiling T_manualvec (140445907610800 for args (ShapedArray(float32[4096,512]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.25492048263549805 sec\n",
            "WARNING:absl:Finished tracing + transforming v_update for jit in 0.026103496551513672 sec\n",
            "WARNING:absl:Compiling v_update (140428908119920 for args (ShapedArray(float32[4096,512]),).\n",
            "WARNING:absl:Finished XLA compilation of v_update in 0.27561426162719727 sec\n",
            "WARNING:absl:Finished tracing + transforming _linspace for jit in 0.006505489349365234 sec\n",
            "WARNING:absl:Compiling _linspace (140428925305872 for args (ShapedArray(float32[], weak_type=True), ShapedArray(int32[], weak_type=True)).\n",
            "WARNING:absl:Finished XLA compilation of _linspace in 0.06710243225097656 sec\n",
            "WARNING:absl:Finished tracing + transforming <lambda> for jit in 0.0004940032958984375 sec\n",
            "WARNING:absl:Compiling <lambda> (140445903003088 for args (ShapedArray(float32[1024]),).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "it works\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Finished XLA compilation of <lambda> in 0.10049295425415039 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.000308990478515625 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003218650817871094 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003070831298828125 sec\n",
            "WARNING:absl:Compiling prim_fun (140428925178320 for args (ShapedArray(float32[]),).\n",
            "WARNING:absl:Finished XLA compilation of broadcast_in_dim in 0.06606817245483398 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0004177093505859375 sec\n",
            "WARNING:absl:Compiling prim_fun (140428925178320 for args (ShapedArray(float32[1024,1024]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.002988100051879883 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.0003352165222167969 sec\n",
            "WARNING:absl:Compiling prim_fun (140428907863888 for args (ShapedArray(float32[8192]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0029296875 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00036263465881347656 sec\n",
            "WARNING:absl:Compiling prim_fun (140428907863888 for args (ShapedArray(float32[1024]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.0026056766510009766 sec\n",
            "WARNING:absl:Finished tracing + transforming prim_fun for jit in 0.00038361549377441406 sec\n",
            "WARNING:absl:Compiling prim_fun (140428907989808 for args (ShapedArray(float32[8192]),).\n",
            "WARNING:absl:Finished XLA compilation of reshape in 0.00241851806640625 sec\n",
            "WARNING:absl:Finished tracing + transforming T_manualvec for jit in 0.0060787200927734375 sec\n",
            "WARNING:absl:Compiling T_manualvec (140428907987120 for args (ShapedArray(float32[8192,1024]),).\n",
            "WARNING:absl:Finished XLA compilation of T_manualvec in 0.40097594261169434 sec\n",
            "WARNING:absl:Finished tracing + transforming v_update for jit in 0.02709054946899414 sec\n",
            "WARNING:absl:Compiling v_update (140428907888720 for args (ShapedArray(float32[8192,1024]),).\n",
            "WARNING:absl:Finished XLA compilation of v_update in 0.4282653331756592 sec\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "it works\n",
            "{'Size of grid': [131072, 524288, 2097152, 8388608], 'Manual Vectorization': [0.37764132200004497, 2.5571336220000376, 19.766007327999887, 157.03423161800015], 'Automatic Vectorization': [0.4339618720000544, 3.02273190599999, 31.13093618799985, 237.56771815899992], 'TPU Parallelization': []}\n"
          ]
        }
      ],
      "source": [
        "results_dict = {\"Size of grid\": [], \"Manual Vectorization\": [], \"Automatic Vectorization\": [], \"TPU Parallelization\": []}\n",
        "for scale in [1,2,4,8]:\n",
        "  # grid for assets\n",
        "  a_size = ap_size = 1024*scale\n",
        "  a_grid = jnp.linspace(a_min, a_max, a_size)  # grid for a\n",
        "  ap_grid = jnp.linspace(a_min, a_max, a_size)                 # grid for a'\n",
        "  #grid for y (use QuantEcon's tauchen() function to create Markov Chains out of AR(1))\n",
        "  Ï = 0.9\n",
        "  Ïƒ = 0.1\n",
        "  y_size = 128*scale\n",
        "  mc = qe.tauchen(Ï, Ïƒ, n=y_size)\n",
        "  y_grid = jnp.exp(mc.state_values)\n",
        "  P = jnp.array(mc.P)\n",
        "  results_dict[\"Size of grid\"].append(a_size*y_size)\n",
        "\n",
        "  grids = {\n",
        "    \"a\": a_grid,\n",
        "    \"y\": y_grid,\n",
        "    \"ap\": ap_grid,}\n",
        "\n",
        "  params = {\n",
        "      \"R\": 1.1,\n",
        "      \"beta\": 0.99,\n",
        "      \"gamma\": 2.5\n",
        "  }\n",
        "\n",
        "  model = {\"params\": params, \n",
        "          \"grids\": grids, \n",
        "          \"Trans_matrix\": P, \n",
        "          \"indices\": {\"a\": jnp.array(range(a_size)), \"y\": jnp.array(range(y_size)), \"ap\": jnp.array(range(ap_size))}}\n",
        "\n",
        "\n",
        "  # initial value\n",
        "  v_init = jnp.zeros((a_size, y_size))\n",
        "  P = jnp.reshape(P, (y_size, y_size, 1))\n",
        "  a = jnp.reshape(a_grid, (a_size, 1, 1))\n",
        "  y = jnp.reshape(y_grid, (1, y_size, 1))\n",
        "  ap = jnp.reshape(ap_grid, (1, 1, ap_size))\n",
        "\n",
        "\n",
        "  T_manualvec_jit = jax.jit(T_manualvec).lower(v_init).compile()\n",
        "  T_autovec = get_update_fn_autovec(model)\n",
        "  T_autovec_jit = jax.jit(T_autovec).lower(v_init).compile()\n",
        "\n",
        "  if use_TPU:\n",
        "    a_partitions = jnp.reshape(model[\"indices\"][\"a\"], (n_devices, a_size//n_devices))\n",
        "    T_tpu = get_update_fn_tpu(model)\n",
        "    T_tpu_jit = jax.pmap(T_tpu, in_axes = (0,None)).lower(a_partitions, v_init).compile()\n",
        "    results_dict[\"Manual Vectorization\"].append(timeit.timeit('T_manualvec_jit(v_init).block_until_ready()', globals=globals(), number=100))\n",
        "    results_dict[\"Automatic Vectorization\"].append(timeit.timeit('T_autovec_jit(v_init).block_until_ready()', globals=globals(), number=100))\n",
        "    results_dict[\"TPU Parallelization\"].append(timeit.timeit('T_tpu_jit(a_partitions, v_init).block_until_ready()', globals=globals(), number=100))\n",
        "  else:\n",
        "    results_dict[\"Manual Vectorization\"].append(timeit.timeit(\"T_manualvec_jit(v_init).block_until_ready()\", globals=globals(), number=100))\n",
        "    results_dict[\"Automatic Vectorization\"].append(timeit.timeit('T_autovec_jit(v_init).block_until_ready()', globals=globals(), number=100))\n",
        "\n",
        "print(results_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d56TPREHHvC3"
      },
      "source": [
        " For TPU, we get {'Manual Vectorization': [0.30772871099907206, 1.4740470310025557, 9.87848552899959, 77.9623624650012], 'Automatic Vectorization': [0.3273367039982986, 1.5696243140009756, 12.601403237997147, 92.91537422500187], 'TPU Parallelization': [0.8347508730003028, 1.4450628260019585, 3.4721897010022076, 17.260116208999534]}\n",
        "\n",
        " For GPU{'Size of grid': [131072, 524288, 2097152, 8388608], 'Manual Vectorization': [0.37764132200004497, 2.5571336220000376, 19.766007327999887, 157.03423161800015], 'Automatic Vectorization': [0.4339618720000544, 3.02273190599999, 31.13093618799985, 237.56771815899992]}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyO9aSSx9nznG6k1q06yS/TL",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "VFI using JAX.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.7 64-bit ('3.8.7')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "0453aa8d5620a3d6221d77b01ecd47dad07bdea904f95f4523430417803ea317"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
